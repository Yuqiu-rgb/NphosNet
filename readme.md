NphosNet: A Deep Learning Model for Predicting Protein N-Phosphorylation Sites Based on Extended Long Short-Term Memory (xLSTM) and Enhanced Features with a Weighted Three-Channel Cross-Attention Strategy

**1.Requirements**

​	pandas
	scikit-learn
	transformers
	matplotlib
	umap-learn

Please download torch version>=2.0 or above to avoid version conflicts

Please refer to the download version of xlstm to **https://github.com/NX-AI/xlstm**

**2.Data**

​	I will upload the data to "/data" directory or OneDrive, or readers can contact me via email 2359448439@qq.com

**3.ProtT5 and EMBERE2 embeeding** 

​	You may also refer to **https://github.com/agemagician/ProtTrans** for detailed explanations.

​	You may also refer to **https://github.com/kWeissenow/EMBER2** for detailed explanations.

​	And place the weights of ProtT5 in the "**prot_t5-xx_half_unref50 enc**" directory, or you can directly place the generated embedding representation in the "**embedding**" directory, and the embeddings generated by EMBER2 are also placed in the "**embedding**" directory.

**4.Demo**

​	Before training, please ensure that the data and corresponding embedding are in the corresponding directory.

​	The read_data.py file contains the code to read three sites.

​	In train.py, there is no need for complex command line operations, just in Python train.py. When switching sites, you need to restore the commented:

```
'''PH'''
    # train, test = data_read_ph()
    # x_train, x_test = train.iloc[:, 1], test.iloc[:, 1]
    # train_label, test_label = train.iloc[:, 0], test.iloc[:, 0]
    #
    # x_train_encoding = BERT_encoding(x_train, x_test).to(device, non_blocking=True)
    # x_test_encoding = BERT_encoding(x_test, x_train).to(device, non_blocking=True)
    #
    # x_train_embedding, x_test_embedding = embedding_load_ph()
    # x_train_str_embedding, x_test_str_embedding = embedding_str_load_ph()
'''PR '''
    train, test = data_read_pr()
    x_train, x_test = train.iloc[:, 1], test.iloc[:, 1]
    train_label, test_label = train.iloc[:, 0], test.iloc[:, 0]

    x_train_encoding = BERT_encoding(x_train, x_test).to(device, non_blocking=True)
    x_test_encoding = BERT_encoding(x_test, x_train).to(device, non_blocking=True)

    x_train_embedding, x_test_embedding = embedding_load_pr()
    x_train_str_embedding, x_test_str_embedding = embedding_str_load_pr()
```

```python
python train.py
```

​	In model_evaluate.py, this code block is used for model evaluation, and only python model_evaluate.py is required. For switching site operations, see train.py

```
python model_evaluation.py
```

