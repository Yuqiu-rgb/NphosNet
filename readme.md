NphosNet: A Deep Learning Model for Predicting Protein N-Phosphorylation Sites Based on Extended Long Short-Term Memory (xLSTM) and Enhanced Features with a Weighted Three-Channel Cross-Attention Strategy

------

The codebase is currently undergoing active refactoring to enhance research reproducibility and clarify the internal data flow architecture of the model, though this process requires additional development time.

Due to GitHub's file size limitations , we are temporarily unable to host the complete dataset along with its corresponding embeddings and model weights. A representative dataset sample has been provisionally archived in the data/ directory. Upon completion of the codebase restructuring, we will systematically organize and upload the full dataset with precomputed embeddings and trained model weights to a dedicated cloud repository **(OneDrive**), with persistent access links embedded in this GitHub repository.

------

**1.Requirements**

​	pandas
	scikit-learn
	transformers
	matplotlib
	umap-learn

Please download torch version>=2.0 or above to avoid version conflicts

Please refer to the download version of xlstm to **https://github.com/NX-AI/xlstm**

The model weights are stored in the **model** directory, UMAP visualization figures in PDF format are contained in the **figures/umap_pdf** directory, and the source code is maintained in the **src** directory.

**2.Data**

​	I will upload the data to "/data" directory of OneDrive

**3.ProtT5 and EMBERE2 Embeeding** 

​	You may also refer to **https://github.com/agemagician/ProtTrans** for detailed explanations.

​	You may also refer to **https://github.com/kWeissenow/EMBER2** for detailed explanations.

​	And place the weights of ProtT5 in the "**prot_t5-xx_half_unref50 enc**" directory, or you can directly place the generated embedding representation in the "**embedding**" directory, and the embeddings generated by EMBER2 are also placed in the "**embedding**" directory.

**4.Demo**

​	Before training, please ensure that the data and corresponding embedding are in the corresponding directory.

​	The read_data.py file contains the code to read three sites.

​	In train.py, there is no need for complex command line operations, just in Python train.py. When switching sites, you need to restore the commented:

```
'''PH'''
    # train, test = data_read_ph()
    # x_train, x_test = train.iloc[:, 1], test.iloc[:, 1]
    # train_label, test_label = train.iloc[:, 0], test.iloc[:, 0]
    #
    # x_train_encoding = BERT_encoding(x_train, x_test).to(device, non_blocking=True)
    # x_test_encoding = BERT_encoding(x_test, x_train).to(device, non_blocking=True)
    #
    # x_train_embedding, x_test_embedding = embedding_load_ph()
    # x_train_str_embedding, x_test_str_embedding = embedding_str_load_ph()
'''PR '''
    train, test = data_read_pr()
    x_train, x_test = train.iloc[:, 1], test.iloc[:, 1]
    train_label, test_label = train.iloc[:, 0], test.iloc[:, 0]

    x_train_encoding = BERT_encoding(x_train, x_test).to(device, non_blocking=True)
    x_test_encoding = BERT_encoding(x_test, x_train).to(device, non_blocking=True)

    x_train_embedding, x_test_embedding = embedding_load_pr()
    x_train_str_embedding, x_test_str_embedding = embedding_str_load_pr()
```

```python
python train.py
```

​	在model_evaluate.py中，此代码块是模型评估时所用，也只需 python model_evaluate.py,切换位点操作见train.py

```
python model_evaluation.py
```

------

